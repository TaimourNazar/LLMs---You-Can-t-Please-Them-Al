{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":83035,"databundleVersionId":10369658,"sourceType":"competition"},{"sourceId":104623,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":72254,"modelId":76277}],"dockerImageVersionId":30805,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/taimour/gemma2-ai-essay-writer-fully-explained?scriptVersionId=211490880\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"<h2>ü§ñ The AI Judge powered by LLMs is ready to check your essays. Can you please him?</h2>\n\n![](https://i.postimg.cc/nhz6whS1/Gemini-Generated-Image-lrkr3blrkr3blrkr.jpg)\n<br/>\n<label>Image generated with Gemini's Imagen 3</label><br/>\n# <span style=\"background-color:#d4fad6;color:black;padding:10px;border-radius:40px;\">ü•Ö Competition Goal</span>\n<div style=\"background-color:white;color:black;padding:20px;border:5px solid brown;border-radius:20px;\">\nImagine you have some robot teachers (judges) who grade essays. These robot teacher (judges) uses a super smart AI (called a Large Language Model or LLM) to decide quality of your essay.\n<br/><hr/>\n<strong>Challenge:</strong>\n<ul>\n    <li>\n<strong>Trick the robot teachers (judges):</strong> Write an essay that makes the robot teachers (judges) disagree with each other on quality of the essay.</li>\n<li><strong>Objective:</strong> Write an essay that's so tricky, that we can mazimie the disagreement between different robot teachers (judges). \nBy doing this, we will help understand how well these AI based LLM systems work and where they might make mistakes.</li>\n</div>\n\n# <span style=\"background-color:#d4fad6;color:black;padding:10px;border-radius:40px;\">üéí Import Libraries</span>","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nfrom transformers import AutoTokenizer, AutoModelForCausalLM","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-06T04:37:40.402594Z","iopub.execute_input":"2024-12-06T04:37:40.403551Z","iopub.status.idle":"2024-12-06T04:37:40.407637Z","shell.execute_reply.started":"2024-12-06T04:37:40.403513Z","shell.execute_reply":"2024-12-06T04:37:40.406748Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"# <span style=\"background-color:#d4fad6;color:black;padding:10px;border-radius:40px;\">‚¨ÜÔ∏è Load Data</span>","metadata":{}},{"cell_type":"code","source":"test = pd.read_csv('/kaggle/input/llms-you-cant-please-them-all/test.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T04:37:40.410382Z","iopub.execute_input":"2024-12-06T04:37:40.410637Z","iopub.status.idle":"2024-12-06T04:37:40.428052Z","shell.execute_reply.started":"2024-12-06T04:37:40.410613Z","shell.execute_reply":"2024-12-06T04:37:40.427293Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"# <span style=\"background-color:#d4fad6;color:black;padding:10px;border-radius:40px;\">üîé View Data</span>","metadata":{}},{"cell_type":"code","source":"test.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T04:37:40.429344Z","iopub.execute_input":"2024-12-06T04:37:40.429628Z","iopub.status.idle":"2024-12-06T04:37:40.44171Z","shell.execute_reply.started":"2024-12-06T04:37:40.429603Z","shell.execute_reply":"2024-12-06T04:37:40.440825Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"        id                                              topic\n0  1097671  Compare and contrast the importance of self-re...\n1  1726150  Evaluate the effectiveness of management consu...\n2  3211968  Discuss the role of self-reliance in achieving...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>topic</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1097671</td>\n      <td>Compare and contrast the importance of self-re...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1726150</td>\n      <td>Evaluate the effectiveness of management consu...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3211968</td>\n      <td>Discuss the role of self-reliance in achieving...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":10},{"cell_type":"markdown","source":"# <span style=\"background-color:#d4fad6;color:black;padding:10px;border-radius:40px;\">‚≠ê Tokenizer & Language Model</span>","metadata":{}},{"cell_type":"code","source":"model_path = '/kaggle/input/gemma-2/transformers/gemma-2-2b-it/2'\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForCausalLM.from_pretrained(model_path,device_map=\"auto\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T04:37:40.442926Z","iopub.execute_input":"2024-12-06T04:37:40.443743Z","iopub.status.idle":"2024-12-06T04:37:48.361509Z","shell.execute_reply.started":"2024-12-06T04:37:40.443705Z","shell.execute_reply":"2024-12-06T04:37:48.360831Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48760529dc3a4e1f9adee0855319d12a"}},"metadata":{}}],"execution_count":11},{"cell_type":"markdown","source":"<div style=\"background-color:white;color:black;padding:20px;border:5px solid blue;border-radius:20px;\">\n    <ul>\n        <li><strong>\n    AutoTokenizer:</strong> This class is used to tokenize input text into a format suitable for the language model.</li>\n<li><strong>AutoModelForCausalLM:</strong> This class represents a causal language model, which is a type of model that generates text sequentially, one token at a time.</li>\n    </ul>\n    </div>","metadata":{}},{"cell_type":"markdown","source":"# <span style=\"background-color:#d4fad6;color:black;padding:10px;border-radius:40px;\">üñãÔ∏è Write Essays</span>","metadata":{}},{"cell_type":"code","source":"essays = []\n\nfor i, row in test.iterrows():\n    prompt = f\"Write a paragraph on the topic {row['topic']}. Sentences should be like 'It is good for this but bad for that'\"\n    input_ids = tokenizer(prompt, return_tensors=\"pt\")\n    input_ids = {k: v.to('cuda') for k, v in input_ids.items()}\n    outputs = model.generate(**input_ids, max_new_tokens=100)\n    essay_text = tokenizer.decode(outputs[0][len(input_ids['input_ids'][0]):])\n    essays.append(essay_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T04:37:48.364033Z","iopub.execute_input":"2024-12-06T04:37:48.364374Z","iopub.status.idle":"2024-12-06T04:47:29.372611Z","shell.execute_reply.started":"2024-12-06T04:37:48.364346Z","shell.execute_reply":"2024-12-06T04:47:29.371884Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"<div style=\"background-color:white;color:black;padding:20px;border:5px solid blue;border-radius:20px;\">\n        This code snippet utilizes the previously loaded model and tokenizer to generate essays on various topics. Let's break it down step by step:\n    <ol>\n<li><strong>\nLooping through Data:</strong>\n\n`for i, row in test.iterrows():` This line iterates through each row in a pandas dataframe named test. The i variable acts as an index, and row represents the current row of data.\n</li><li><strong>\nCreating Input Text:</strong>\n\n`prompt = f\"Write prompt here with topic {row['topic']}\":` This line constructs the prompt for the language model. It combines the prompt with the topic retrieved from the current row using row['topic'].\n</li><li><strong>\nTokenization and Conversion:</strong>\n\n`*input_ids = tokenizer(prompt, return_tensors=\"pt\")*:` This line uses the loaded tokenizer to convert the input_text into numerical tokens (input_ids). The return_tensors=\"pt\" argument specifies that the output should be converted to a PyTorch tensor for compatibility with the model.\n</li><li><strong>\nMoving to GPU (Optional):</strong>\n\n`(input_ids = {k: v.to('cuda') for k, v in input_ids.items()})` assumes you have a GPU available. It iterates through each key-value pair in the input_ids dictionary and moves the values (tensors) to the CUDA device if possible. This improves processing speed if a GPU is available.\n</li><li><strong>\nGenerating Text:</strong>\n\n`essay_text = model.generate(**input_ids, max_new_tokens=number):` This line utilizes the loaded model (model) to generate text. It unpacks the input_ids dictionary and passes its contents as keyword arguments to the generate method. The max_new_tokens=number argument sets the maximum number of new tokens the model can generate.\n</li><li><strong>\nDecoding Output:</strong>\n\n`output_text = tokenizer.decode(essay_text[0][len(input_ids['input_ids'][0]):]):` This line decodes the generated output (outputs[0]) back into human-readable text using the tokenizer. However, it only decodes the newly generated portion (excluding the prompt). The slicing [len(input_ids['input_ids'][0]):] ensures this behavior. In this way we get only the generated essay by model.\n</li><li><strong>\nStoring Essays:</strong>\n\n`essays.append(output_text):` This line appends the decoded essay (output_text) to a list named essays. This list will eventually contain essays generated on all topics from the test_data dataframe.\nIn essence, this code iterates through your test data, uses the model to generate essays based on the provided topics, and stores them in a list for further use.\n</li>\n    </ul>\n    </div>","metadata":{}},{"cell_type":"markdown","source":"# <span style=\"background-color:#d4fad6;color:black;padding:10px;border-radius:40px;\">üìÅ Submission</span>","metadata":{}},{"cell_type":"code","source":"test['essay'] = essays\ntest=test.drop('topic', axis=1)\ntest.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T04:47:29.373699Z","iopub.execute_input":"2024-12-06T04:47:29.373997Z","iopub.status.idle":"2024-12-06T04:47:29.383685Z","shell.execute_reply.started":"2024-12-06T04:47:29.373965Z","shell.execute_reply":"2024-12-06T04:47:29.382818Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"        id                                              essay\n0  1097671   or 'While this is important, it can be detrim...\n1  1726150   and 'While it can be effective, it often come...\n2  3211968   and 'This can be helpful but...' \\n\\nIt is go...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>essay</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1097671</td>\n      <td>or 'While this is important, it can be detrim...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1726150</td>\n      <td>and 'While it can be effective, it often come...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3211968</td>\n      <td>and 'This can be helpful but...' \\n\\nIt is go...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"test.to_csv('submission.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T04:47:29.384846Z","iopub.execute_input":"2024-12-06T04:47:29.385178Z","iopub.status.idle":"2024-12-06T04:47:29.399117Z","shell.execute_reply.started":"2024-12-06T04:47:29.385144Z","shell.execute_reply":"2024-12-06T04:47:29.398392Z"}},"outputs":[],"execution_count":14}]}